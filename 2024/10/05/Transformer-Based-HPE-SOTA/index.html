<!DOCTYPE html>
<html lang="en">
    <!-- title -->
<!-- keywords -->
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="author" content="Yan Zhang">
    <meta name="renderer" content="webkit">
    <meta name="copyright" content="Yan Zhang">
        <meta name="keywords" content="hexo,hexo-theme,hexo-blog">
    <meta name="description" content="">
    <meta name="description" content="原创性声明本文为作者原创，在个人Blog首次发布，如需转载请注明引用出处。（&amp;#121;&amp;#x61;&amp;#110;&amp;#122;&amp;#x68;&amp;#x61;&amp;#x6e;&amp;#103;&amp;#x2e;&amp;#99;&amp;#x67;&amp;#x40;&amp;#103;&amp;#109;&amp;#97;&amp;#x69;&amp;#108;&amp;#46;&amp;#99;&amp;#x6f;&amp;#x6d; 或 https:&#x2F;&#x2F;graphicyan.github.io&#x2F;） 一、引言前两">
<meta property="og:type" content="article">
<meta property="og:title" content="基于Transformer的人体姿态重建（三）：几种SOTA工作的详细解析与未来展望">
<meta property="og:url" content="https://graphicyan.github.io/2024/10/05/Transformer-Based-HPE-SOTA/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="原创性声明本文为作者原创，在个人Blog首次发布，如需转载请注明引用出处。（&amp;#121;&amp;#x61;&amp;#110;&amp;#122;&amp;#x68;&amp;#x61;&amp;#x6e;&amp;#103;&amp;#x2e;&amp;#99;&amp;#x67;&amp;#x40;&amp;#103;&amp;#109;&amp;#97;&amp;#x69;&amp;#108;&amp;#46;&amp;#99;&amp;#x6f;&amp;#x6d; 或 https:&#x2F;&#x2F;graphicyan.github.io&#x2F;） 一、引言前两">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2024-10-05T07:56:53.000Z">
<meta property="article:modified_time" content="2025-07-20T14:21:04.779Z">
<meta property="article:author" content="Yan Zhang">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">
    <meta http-equiv="Cache-control" content="no-cache">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <link rel="icon" href="/intro/yan.jpg">
    <title>基于Transformer的人体姿态重建（三）：几种SOTA工作的详细解析与未来展望 · Yan&#39;s World</title>
    <!-- /*! loadCSS. [c]2017 Filament Group, Inc. MIT License */
/* This file is meant as a standalone workflow for
- testing support for link[rel=preload]
- enabling async CSS loading in browsers that do not support rel=preload
- applying rel preload css once loaded, whether supported or not.
*/ -->
<script>
    (function (w) {
        'use strict'
        // rel=preload support test
        if (!w.loadCSS) {
            w.loadCSS = function () {}
        }
        // define on the loadCSS obj
        var rp = (loadCSS.relpreload = {})
        // rel=preload feature support test
        // runs once and returns a function for compat purposes
        rp.support = (function () {
            var ret
            try {
                ret = w.document.createElement('link').relList.supports('preload')
            } catch (e) {
                ret = false
            }
            return function () {
                return ret
            }
        })()

        // if preload isn't supported, get an asynchronous load by using a non-matching media attribute
        // then change that media back to its intended value on load
        rp.bindMediaToggle = function (link) {
            // remember existing media attr for ultimate state, or default to 'all'
            var finalMedia = link.media || 'all'

            function enableStylesheet() {
                link.media = finalMedia
            }

            // bind load handlers to enable media
            if (link.addEventListener) {
                link.addEventListener('load', enableStylesheet)
            } else if (link.attachEvent) {
                link.attachEvent('onload', enableStylesheet)
            }

            // Set rel and non-applicable media type to start an async request
            // note: timeout allows this to happen async to let rendering continue in IE
            setTimeout(function () {
                link.rel = 'stylesheet'
                link.media = 'only x'
            })
            // also enable media after 3 seconds,
            // which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
            setTimeout(enableStylesheet, 3000)
        }

        // loop through link elements in DOM
        rp.poly = function () {
            // double check this to prevent external calls from running
            if (rp.support()) {
                return
            }
            var links = w.document.getElementsByTagName('link')
            for (var i = 0; i < links.length; i++) {
                var link = links[i]
                // qualify links to those with rel=preload and as=style attrs
                if (
                    link.rel === 'preload' &&
                    link.getAttribute('as') === 'style' &&
                    !link.getAttribute('data-loadcss')
                ) {
                    // prevent rerunning on link
                    link.setAttribute('data-loadcss', true)
                    // bind listeners to toggle media back
                    rp.bindMediaToggle(link)
                }
            }
        }

        // if unsupported, run the polyfill
        if (!rp.support()) {
            // run once at least
            rp.poly()

            // rerun poly on an interval until onload
            var run = w.setInterval(rp.poly, 500)
            if (w.addEventListener) {
                w.addEventListener('load', function () {
                    rp.poly()
                    w.clearInterval(run)
                })
            } else if (w.attachEvent) {
                w.attachEvent('onload', function () {
                    rp.poly()
                    w.clearInterval(run)
                })
            }
        }

        // commonjs
        if (typeof exports !== 'undefined') {
            exports.loadCSS = loadCSS
        } else {
            w.loadCSS = loadCSS
        }
    })(typeof global !== 'undefined' ? global : this)
</script>

    <style type="text/css">
    @font-face {
        font-family: 'Oswald-Regular';
        src: url("/font/Oswald-Regular.ttf");
    }

    body {
        margin: 0;
    }

    header,
    footer,
    .footer-fixed-btn,
    .sidebar,
    .container,
    .site-intro-meta,
    .toc-wrapper {
        display: none;
    }

    .site-intro {
        position: relative;
        z-index: 3;
        width: 100%;
        /* height: 50vh; */
        overflow: hidden;
    }

    .site-intro-placeholder {
        position: absolute;
        z-index: -2;
        top: 0;
        left: 0;
        width: calc(100% + 300px);
        height: 100%;
        background: repeating-linear-gradient(
            -45deg,
            #444 0,
            #444 80px,
            #333 80px,
            #333 160px
        );
        background-position: center center;
        transform: translate3d(-226px, 0, 0);
        animation: gradient-move 2.5s ease-out 0s infinite;
    }

    @keyframes gradient-move {
        0% {
            transform: translate3d(-226px, 0, 0);
        }
        100% {
            transform: translate3d(0, 0, 0);
        }
    }
</style>

    <link id="stylesheet-fancybox" rel="preload" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.36/dist/fancybox/fancybox.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <link id="stylesheet-base" rel="preload" href="/css/style.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <link id="stylesheet-mobile" rel="preload" href="/css/mobile.css" as="style" onload="this.onload=null;this.rel='stylesheet';this.media='screen and (max-width: 960px)'">
    <link id="stylesheet-theme-dark" rel="preload" href="/css/dark.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" as="script">
    <link rel="preload" href="/scripts/main.js" as="script">
    <link rel="preload" href="/font/Oswald-Regular.ttf" as="font" crossorigin>
    <link rel="preload" href="https://at.alicdn.com/t/font_327081_1dta1rlogw17zaor.woff" as="font" crossorigin>
    <!-- algolia -->
    <!-- 百度统计  -->
    <!-- 谷歌统计  -->
    <!-- Google tag (gtag.js) -->
<meta name="generator" content="Hexo 6.3.0"></head>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script>
    <script type="text/javascript">
        if (typeof window.$ == undefined) {
            console.warn('jquery load from jsdelivr failed, will load local script')
            document.write('<script src="/lib/jquery.min.js" />')
        }
    </script>
        <body class="post-body">
        <!-- header -->
        <header class="header header-mobile">
    <!-- top read progress line -->
    <div class="header-element">
        <div class="read-progress"></div>
    </div>
    <!-- sidebar menu button -->
    <div class="header-element">
        <div class="header-sidebar-menu">
            <div style="padding-left: 1px;">&#xe775;</div>
        </div>
    </div>
    <!-- header actions -->
    <div class="header-actions">
        <!-- theme mode switch button -->
        <span class="header-theme-btn header-element">
            <i class="fas fa-adjust"></i>
        </span>
        <!-- back to home page text -->
        <span class="home-link header-element">
            <a href="/">Yan's World.</a>
        </span>
    </div>
    <!-- toggle banner -->
    <div class="banner">
        <div class="blog-title header-element">
            <a href="/">Yan&#39;s World.</a>
        </div>
        <div class="post-title header-element">
            <a href="#" class="post-name">基于Transformer的人体姿态重建（三）：几种SOTA工作的详细解析与未来展望</a>
        </div>
    </div>
</header>

        <!-- fixed footer -->
        <footer class="footer-fixed">
    <!-- donate button -->

    <!-- back to top button -->
    <div class="footer-fixed-btn footer-fixed-btn--hidden back-top">
        <div>&#xe639;</div>
    </div>
</footer>

        <!-- wrapper -->
        <div class="wrapper">
            <div class="site-intro" style="    height:50vh;
">
    <!-- 主页  -->
    <!-- 404页  -->
    <div class="site-intro-placeholder"></div>
    <div class="site-intro-img" style="background-image: url(/intro/post-bg.jpg)"></div>
    <div class="site-intro-meta">
        <!-- 标题  -->
        <h1 class="intro-title">
            <!-- 主页  -->
                基于Transformer的人体姿态重建（三）：几种SOTA工作的详细解析与未来展望
            <!-- 404 -->
        </h1>
        <!-- 副标题 -->
        <p class="intro-subtitle">
            <!-- 主页副标题  -->
            <!-- 404 -->
        </p>
        <!-- 文章页 meta -->
            <div class="post-intros">
                <!-- 文章页标签  -->
                    <div class="post-intro-tags" >
        <a class="post-tag" href="javascript:void(0);" data-tags="AI">AI</a>
</div>

                <!-- 文章字数统计 -->
                    <div class="post-intro-read">
                        <span>Word count: <span class="post-count word-count">3.3k</span>Reading time: <span class="post-count reading-time">11 min</span></span>
                    </div>
                <div class="post-intro-meta">
                    <!-- 撰写日期 -->
                    <span class="iconfont-archer post-intro-calander">&#xe676;</span>
                    <span class="post-intro-time">2024/10/05</span>
                    <!-- busuanzi -->
                        <span id="busuanzi_container_page_pv" class="busuanzi-pv">
                            <span class="iconfont-archer post-intro-busuanzi">&#xe602;</span>
                            <span id="busuanzi_value_page_pv"></span>
                        </span>
                    <!-- 文章分享 -->
                    <span class="share-wrapper">
                        <span class="iconfont-archer share-icon">&#xe71d;</span>
                        <span class="share-text">Share</span>
                        <ul class="share-list">
                            <li class="iconfont-archer share-qr" data-type="qr">&#xe75b;
                                <div class="share-qrcode"></div>
                            </li>
                            <li class="iconfont-archer" data-type="weibo">&#xe619;</li>
                            <li class="iconfont-archer" data-type="qzone">&#xe62e;</li>
                            <li class="iconfont-archer" data-type="twitter">&#xe634;</li>
                            <li class="iconfont-archer" data-type="facebook">&#xe67a;</li>
                        </ul>
                    </span>
                </div>
            </div>
    </div>
</div>

            <script>
  // get user agent
  function getBrowserVersions() {
    var u = window.navigator.userAgent
    return {
      userAgent: u,
      trident: u.indexOf('Trident') > -1, //IE内核
      presto: u.indexOf('Presto') > -1, //opera内核
      webKit: u.indexOf('AppleWebKit') > -1, //苹果、谷歌内核
      gecko: u.indexOf('Gecko') > -1 && u.indexOf('KHTML') == -1, //火狐内核
      mobile: !!u.match(/AppleWebKit.*Mobile.*/), //是否为移动终端
      ios: !!u.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/), //ios终端
      android: u.indexOf('Android') > -1 || u.indexOf('Linux') > -1, //android终端或者uc浏览器
      iPhone: u.indexOf('iPhone') > -1 || u.indexOf('Mac') > -1, //是否为iPhone或者安卓QQ浏览器
      iPad: u.indexOf('iPad') > -1, //是否为iPad
      webApp: u.indexOf('Safari') == -1, //是否为web应用程序，没有头部与底部
      weixin: u.indexOf('MicroMessenger') == -1, //是否为微信浏览器
      uc: u.indexOf('UCBrowser') > -1, //是否为android下的UC浏览器
    }
  }
  var browser = {
    versions: getBrowserVersions(),
  }
  console.log('userAgent: ' + browser.versions.userAgent)

  // callback
  function fontLoaded() {
    console.log('font loaded')
    if (document.getElementsByClassName('site-intro-meta')) {
      document
        .getElementsByClassName('intro-title')[0]
        .classList.add('intro-fade-in')
      document
        .getElementsByClassName('intro-subtitle')[0]
        .classList.add('intro-fade-in')
      var postIntros = document.getElementsByClassName('post-intros')[0]
      if (postIntros) {
        postIntros.classList.add('post-fade-in')
      }
    }
  }

  // UC不支持跨域，所以直接显示
  function asyncCb() {
    if (browser.versions.uc) {
      console.log('UCBrowser')
      fontLoaded()
    } else {
      WebFont.load({
        custom: {
          families: ['Oswald-Regular'],
        },
        loading: function () {
          // 所有字体开始加载
          // console.log('font loading');
        },
        active: function () {
          // 所有字体已渲染
          fontLoaded()
        },
        inactive: function () {
          // 字体预加载失败，无效字体或浏览器不支持加载
          console.log('inactive: timeout')
          fontLoaded()
        },
        timeout: 5000, // Set the timeout to two seconds
      })
    }
  }

  function asyncErr() {
    console.warn('script load from CDN failed, will load local script')
  }

  // load webfont-loader async, and add callback function
  function async(u, cb, err) {
    var d = document,
      t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0]
    o.src = u
    if (cb) {
      o.addEventListener(
        'load',
        function (e) {
          cb(null, e)
        },
        false
      )
    }
    if (err) {
      o.addEventListener(
        'error',
        function (e) {
          err(null, e)
        },
        false
      )
    }
    s.parentNode.insertBefore(o, s)
  }

  var asyncLoadWithFallBack = function (arr, success, reject) {
    var currReject = function () {
      reject()
      arr.shift()
      if (arr.length) async(arr[0], success, currReject)
    }

    async(arr[0], success, currReject)
  }

  asyncLoadWithFallBack(
    [
      'https://cdn.jsdelivr.net/npm/webfontloader@1.6.28/webfontloader.min.js',
      'https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js',
      "/lib/webfontloader.min.js",
    ],
    asyncCb,
    asyncErr
  )
</script>

            <img class="loading" src="/assets/loading.svg" style="display: block; margin: 6rem auto 0 auto; width: 6rem; height: 6rem;" alt="loading">
            <div class="container container-unloaded">
                <main class="main post-page">
    <article class="article-entry">
        <h2 id="原创性声明"><a href="#原创性声明" class="headerlink" title="原创性声明"></a>原创性声明</h2><p>本文为作者原创，在个人Blog首次发布，如需转载请注明引用出处。（<a href="mailto:&#121;&#x61;&#110;&#122;&#x68;&#x61;&#x6e;&#103;&#x2e;&#99;&#x67;&#x40;&#103;&#109;&#97;&#x69;&#108;&#46;&#99;&#x6f;&#x6d;">&#121;&#x61;&#110;&#122;&#x68;&#x61;&#x6e;&#103;&#x2e;&#99;&#x67;&#x40;&#103;&#109;&#97;&#x69;&#108;&#46;&#99;&#x6f;&#x6d;</a> 或 <a href="https://graphicyan.github.io/%EF%BC%89">https://graphicyan.github.io/）</a></p>
<h2 id="一、引言"><a href="#一、引言" class="headerlink" title="一、引言"></a>一、引言</h2><p>前两篇文章中，我准备在3D人形动作领域结合数据和网络的优势来落地一些技术尝试。实际上，随着深度学习技术的发展，尤其是Transformer架构的引入，人体姿态估计领域已经取得了一些显著进展。本文就详细介绍三个基于Transformer的最新SOTA（State-of-the-Art）工作：<strong>HMR2.0</strong>、<strong>Multi-HMR</strong>和<strong>GVHMR</strong>，并按照发表时间顺序进行讨论。每个方法的技术原理、优势、训练数据集及配置都将被详细讲解。</p>
<hr>
<h2 id="二、SOTA方法详解"><a href="#二、SOTA方法详解" class="headerlink" title="二、SOTA方法详解"></a>二、SOTA方法详解</h2><h3 id="1-HMR2-0-2023-05-31"><a href="#1-HMR2-0-2023-05-31" class="headerlink" title="1. HMR2.0 (2023-05-31)"></a>1. HMR2.0 (2023-05-31)</h3><h4 id="核心原理"><a href="#核心原理" class="headerlink" title="核心原理"></a>核心原理</h4><ul>
<li><strong>ViT Backbone</strong>: 引入了Vision Transformer (ViT) 作为特征提取器，替代了传统的卷积神经网络(CNN)，以捕捉更丰富的全局信息。</li>
<li><strong>Temporal Cues</strong>: 利用时间线索直接从视频中预测骨架姿态序列，通过深度网络预测骨骼姿态序列。</li>
<li><strong>End-to-end Learning</strong>: 整个过程从图像输入到SMPL参数输出完全端到端训练，简化了流程，并提升了模型的鲁棒性。</li>
</ul>
<h4 id="技术优势"><a href="#技术优势" class="headerlink" title="技术优势"></a>技术优势</h4><ul>
<li><strong>精度提升</strong>: 在Human3.6M数据集上，MPJPE从原始HMR的58.7mm降至42.3mm。</li>
<li><strong>鲁棒性增强</strong>: 通过Transformer的全局建模能力，显著改善遮挡场景下的预测效果。</li>
<li><strong>轻量化设计</strong>: 尽管精度有所提高，但推理速度并未显著下降。</li>
</ul>
<h4 id="缺点分析"><a href="#缺点分析" class="headerlink" title="缺点分析"></a>缺点分析</h4><ul>
<li><strong>对复杂遮挡处理能力有限</strong>：尽管HMR2.0通过Transformer架构显著提升了对遮挡场景的鲁棒性，但在极端遮挡情况下（如多人重叠或物体遮挡），其表现仍不如预期。</li>
<li><strong>依赖高质量标注数据</strong>：HMR2.0的训练高度依赖于大规模高质量的动作捕捉数据集（如Human3.6M和MPI-INF-3DHP），对于缺乏此类数据的新应用场景，模型泛化能力可能受限。</li>
<li><strong>实时性能有待提升</strong>：虽然HMR2.0在精度上有了显著提升，但为了达到更高的准确度，模型复杂度增加，导致推理速度相对较慢，难以满足某些实时应用的需求。</li>
</ul>
<h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><ul>
<li><strong>训练数据集</strong>:<ul>
<li>**Human3.6M [Ionescu et al. 2014]**：包含专业演员在实验室环境中执行各种动作的高精度3D标注数据。</li>
<li>**MPI-INF-3DHP [Mehta et al. 2017]**：提供多视角的动作捕捉数据，有助于提升模型在复杂场景中的表现。</li>
</ul>
</li>
<li><strong>验证数据集</strong>:<ul>
<li><strong>Human3.6M</strong>: 作为主要验证集，用于评估模型的准确性和鲁棒性。</li>
</ul>
</li>
</ul>
<h4 id="训练配置"><a href="#训练配置" class="headerlink" title="训练配置"></a>训练配置</h4><ul>
<li>使用AdamW优化器，初始学习率为$1e^{-4}$，权重衰减为0.01。</li>
<li>模型在多个GPU上分布式训练，每批次大小为64。</li>
</ul>
<h4 id="性能指标"><a href="#性能指标" class="headerlink" title="性能指标"></a>性能指标</h4><ul>
<li>在Human3.6M上的MPJPE（Mean Per Joint Position Error）从原始HMR的58.7mm降低到42.3mm，显示了显著的精度提升。此外，在MPI-INF-3DHP上也取得了优异的表现，表明其在不同数据集上的泛化能力。</li>
</ul>
<h3 id="2-Multi-HMR-2024-02-22"><a href="#2-Multi-HMR-2024-02-22" class="headerlink" title="2. Multi-HMR (2024-02-22)"></a>2. Multi-HMR (2024-02-22)</h3><h4 id="核心原理-1"><a href="#核心原理-1" class="headerlink" title="核心原理"></a>核心原理</h4><ul>
<li><strong>Patch-level Detection + Offset Regression</strong>: 采用CenterNet范式进行检测阶段处理，允许一次性提出方法而无需复杂的后处理。</li>
<li><strong>Human Perception Head (HPH)</strong>: 每个检测到的token作为查询，使用交叉注意力机制预测姿势和形状参数，以及三维空间位置。</li>
<li><strong>相机参数嵌入</strong>: 可选地，已知的相机内参通过傅里叶编码表示为射线起源于相机中心的信息。</li>
</ul>
<h4 id="技术优势-1"><a href="#技术优势-1" class="headerlink" title="技术优势"></a>技术优势</h4><ul>
<li><strong>高效多人员处理</strong>: 首次实现了单帧中多人全身3D网格重建，无需两步流程（先检测后重建）。</li>
<li><strong>高精度</strong>: 通过ViT提取全局特征，提升了对复杂场景的理解能力。</li>
<li><strong>灵活性</strong>: 支持可选的相机参数输入，增强了模型在不同场景下的适应性。</li>
</ul>
<h4 id="缺点分析-1"><a href="#缺点分析-1" class="headerlink" title="缺点分析"></a>缺点分析</h4><ul>
<li><strong>计算资源需求高</strong>：Multi-HMR使用了ViT作为Backbone，并且支持可选的相机参数输入，这增加了计算复杂度和内存需求。特别是在处理高清图像或多人员场景时，对硬件要求较高。</li>
<li><strong>手部细节优化不足</strong>：尽管引入了CUFFS数据集以改善手部关键点的预测精度，但在实际应用中，手部姿态估计的准确性仍有待进一步提高，尤其是在复杂手势或手部遮挡的情况下。</li>
<li><strong>模型大小与部署灵活性</strong>：由于采用了复杂的网络结构和多种损失函数，Multi-HMR的整体模型尺寸较大，不利于在资源受限设备上的部署。</li>
</ul>
<h4 id="数据集-1"><a href="#数据集-1" class="headerlink" title="数据集"></a>数据集</h4><ul>
<li><strong>训练数据集</strong>:<ul>
<li><strong>MuPoTS, 3DPW, EHF</strong>: 这些数据集主要用于评估3D人体网格恢复性能。</li>
<li><strong>BEDLAM, AGORA, CUFFS and UBody</strong>: 用于训练通用模型的数据集组合，其中UBody包含噪声较大的真实世界图像，有助于提升模型的鲁棒性。特别地，CUFFS是一个专门为改善手部细节而设计的合成数据集。</li>
</ul>
</li>
<li><strong>验证数据集</strong>:<ul>
<li><strong>MuPoTS, CMU, MuCo</strong>: 用于验证模型在多人场景下的性能。</li>
</ul>
</li>
</ul>
<h4 id="训练配置-1"><a href="#训练配置-1" class="headerlink" title="训练配置"></a>训练配置</h4><ul>
<li>使用多种重建损失：直接在SMPL-X参数(rot)，生成的顶点(v3d)，结合两种(rot+v3d)，以及添加重投影损失(+v2d)。</li>
<li>在不同的分辨率下进行了实验（如896×896），并且使用了不同的骨干网络预训练策略（如DINOv2）。</li>
</ul>
<h4 id="性能指标-1"><a href="#性能指标-1" class="headerlink" title="性能指标"></a>性能指标</h4><ul>
<li>在MuPoTS数据集上的PA-MPJPE（Protocol-A Mean Per Joint Position Error）达到了78.8mm，相较于其他方法有明显优势。特别是在处理遮挡问题和多人交互方面，Multi-HMR展现了出色的性能。使用CUFFS数据集进行训练后，对于手部关键点的预测精度有了显著提高。</li>
</ul>
<h3 id="3-GVHMR-2024-09-10"><a href="#3-GVHMR-2024-09-10" class="headerlink" title="3. GVHMR (2024-09-10)"></a>3. GVHMR (2024-09-10)</h3><h4 id="核心原理-2"><a href="#核心原理-2" class="headerlink" title="核心原理"></a>核心原理</h4><ul>
<li><strong>Gravity-View Coordinates</strong>: 定义了一个由重力方向和摄像机视角方向组成的坐标系统，以减少重力方向上的累积误差。</li>
<li><strong>Rotation Prediction</strong>: 相比于全三维自由度旋转，更直观地估计围绕重力方向的一维旋转。</li>
<li><strong>Transformer with Rotary Positional Embedding (RoPE)</strong>: 使用增强的Transformer模型直接回归整个动作序列，更好地捕捉视频帧之间的相对关系，适用于长序列建模。</li>
</ul>
<h4 id="技术优势-2"><a href="#技术优势-2" class="headerlink" title="技术优势"></a>技术优势</h4><ul>
<li><strong>世界坐标系重建</strong>: 实现了无需外部传感器即可恢复与重力对齐的全局运动轨迹。</li>
<li><strong>抗噪声能力</strong>: 通过重力视图坐标减少了相机运动与人体运动的耦合干扰。</li>
<li><strong>实时性能</strong>: RoPE使得模型能够处理无限长序列，并且支持并行推理。</li>
</ul>
<h4 id="数据集-2"><a href="#数据集-2" class="headerlink" title="数据集"></a>数据集</h4><ul>
<li><strong>训练数据集</strong>:<ul>
<li>**AMASS [Mahmood et al. 2019]**：一个大型的人体运动捕捉数据集，主要用于生成合成数据。</li>
<li>**BEDLAM [Black et al. 2023]**：包含丰富的全身运动捕捉数据，有助于提升模型对复杂动作的理解能力。</li>
<li>**H36M [Ionescu et al. 2014]**：高精度的动作捕捉数据集，常用于人体姿态估计任务。</li>
<li>**3DPW [von Marcard et al. 2018]**：提供了真实世界中的多视角人体姿态数据，有助于增强模型的泛化能力。</li>
</ul>
</li>
<li><strong>验证数据集</strong>:<ul>
<li>**RICH [Huang et al. 2022]**：包含静态摄像头捕获的视频序列，总时长为59.1分钟，具有精确的全局人体运动标注。</li>
<li>**EMDB-2 [Kaufmann et al. 2023]**：使用移动摄像头拍摄的数据集，包含25个序列，总时长为24.0分钟。</li>
</ul>
</li>
</ul>
<h4 id="训练配置-2"><a href="#训练配置-2" class="headerlink" title="训练配置"></a>训练配置</h4><ul>
<li>GVHMR拥有12层的Transformer编码器，每个注意力单元有8个头，隐藏维度为512。MLP层使用GELU激活函数。</li>
<li>训练过程中采用了多种数据增强技术，如在AMASS数据集中模拟静态和动态相机轨迹，归一化关键点等。</li>
<li>序列长度设置为L&#x3D;120，经过500个epoch后模型收敛，批次大小为256，整个训练过程大约需要13小时在2块RTX 4090 GPU上完成。</li>
</ul>
<h4 id="性能指标-2"><a href="#性能指标-2" class="headerlink" title="性能指标"></a>性能指标</h4><table>
<thead>
<tr>
<th>模型</th>
<th>数据集</th>
<th>PA-MPJPE</th>
<th>MPJPE</th>
<th>W-MPJPE</th>
<th>WA-MPJPE</th>
</tr>
</thead>
<tbody><tr>
<td>GVHMR</td>
<td>RICH(24)</td>
<td>78.8 mm</td>
<td>126.3 mm</td>
<td>126.3 mm</td>
<td>78.8 mm</td>
</tr>
<tr>
<td>GVHMR</td>
<td>EMDB(24)</td>
<td>111.0 mm</td>
<td>276.5 mm</td>
<td>274.9 mm</td>
<td>110.6 mm</td>
</tr>
</tbody></table>
<p>具体来说，在RICH和EMDB-2数据集上的表现如下：</p>
<ul>
<li>在RICH数据集上，GVHMR的WA-MPJPE（World-Aligned MPJPE）为78.8mm，W-MPJPE（World-coordinate MPJPE）为126.3mm，显示出较高的准确性。</li>
<li>在EMDB-2数据集上，WA-MPJPE为111.0mm，W-MPJPE为276.5mm，同样表现出色，特别是在处理相机移动的情况下，GVHMR能够更准确地捕捉复杂的运动轨迹，并且具有良好的平滑度和稳定性。</li>
</ul>
<h4 id="缺点分析-2"><a href="#缺点分析-2" class="headerlink" title="缺点分析"></a>缺点分析</h4><ul>
<li><strong>重力感知依赖性强</strong>：GVHMR的核心优势在于其能够恢复与重力对齐的全局运动轨迹，但这同时也意味着它对环境中的重力方向非常敏感。如果摄像头或传感器未能正确校准重力方向，可能会导致较大的误差。</li>
<li><strong>长序列建模挑战</strong>：虽然RoPE机制使得GVHMR能够在理论上处理无限长的视频序列，但在实际应用中，随着序列长度增加，模型训练难度和计算成本也随之上升，可能导致训练不稳定或过拟合问题。</li>
<li><strong>数据增强依赖性</strong>：GVHMR在训练过程中广泛使用了数据增强技术（如模拟静态和动态相机轨迹）。然而，过度依赖这些增强手段可能会导致模型在真实世界数据上的泛化能力下降，特别是在面对未见过的数据分布时。</li>
</ul>
<h3 id="4-综合比较"><a href="#4-综合比较" class="headerlink" title="4.综合比较"></a>4.综合比较</h3><table>
<thead>
<tr>
<th>方法</th>
<th>主要优点</th>
<th>主要缺点</th>
</tr>
</thead>
<tbody><tr>
<td>HMR2.0</td>
<td>精度高，鲁棒性好，轻量化设计</td>
<td>对复杂遮挡处理能力有限，依赖高质量标注数据，实时性能有待提升</td>
</tr>
<tr>
<td>Multi-HMR</td>
<td>高效多人员处理，高精度，灵活性强</td>
<td>计算资源需求高，手部细节优化不足，模型大小与部署灵活性差</td>
</tr>
<tr>
<td>GVHMR</td>
<td>世界坐标系重建，抗噪声能力强，实时性能好</td>
<td>重力感知依赖性强，长序列建模挑战大，数据增强依赖性</td>
</tr>
</tbody></table>
<hr>
<h2 id="三、Transformer架构的优势"><a href="#三、Transformer架构的优势" class="headerlink" title="三、Transformer架构的优势"></a>三、Transformer架构的优势</h2><h3 id="1-全局信息捕获"><a href="#1-全局信息捕获" class="headerlink" title="1. 全局信息捕获"></a>1. 全局信息捕获</h3><ul>
<li><strong>对比CNN</strong>: CNN主要依赖局部感受野，难以捕捉长距离关节间的关联，而Transformer通过自注意力机制可以同时考虑所有关节间的关系。</li>
<li><strong>实证效果</strong>: ViTPose等基于Transformer的方法在COCO数据集上的AP指标较HRNet有显著提升。</li>
</ul>
<h3 id="2-并行计算效率"><a href="#2-并行计算效率" class="headerlink" title="2. 并行计算效率"></a>2. 并行计算效率</h3><ul>
<li><strong>硬件友好性</strong>: Transformer的矩阵运算非常适合GPU&#x2F;TPU加速，如ViT-B在NVIDIA A100上可以实现较高的推理速度。</li>
<li><strong>扩展性强</strong>: 可以通过调整层数和通道数灵活调整模型大小以适应不同的应用场景需求。</li>
</ul>
<h3 id="3-灵活的输入输出结构"><a href="#3-灵活的输入输出结构" class="headerlink" title="3. 灵活的输入输出结构"></a>3. 灵活的输入输出结构</h3><ul>
<li><strong>多模态兼容</strong>: Transformer可以轻松处理多种类型的输入（如RGB图像、深度图、IMU数据），并通过SMPLX等参数化模型生成3D网格或关键点输出。</li>
</ul>
<h3 id="4-自监督预训练潜力"><a href="#4-自监督预训练潜力" class="headerlink" title="4. 自监督预训练潜力"></a>4. 自监督预训练潜力</h3><ul>
<li><strong>MAE&#x2F;BEiT应用</strong>: 利用掩码图像重建等自监督策略预训练Transformer，减少对标注数据的依赖，从而提升模型泛化能力。</li>
</ul>
<hr>
<h2 id="四、未来发展方向及改进建议"><a href="#四、未来发展方向及改进建议" class="headerlink" title="四、未来发展方向及改进建议"></a>四、未来发展方向及改进建议</h2><ol>
<li><p><strong>改进遮挡处理能力</strong>：</p>
<ul>
<li>引入更多先进的遮挡处理算法，例如利用多视角信息进行融合，或者采用生成对抗网络（GANs）来生成缺失部分的人体姿态。</li>
</ul>
</li>
<li><p><strong>减少对高质量标注数据的依赖</strong>：</p>
<ul>
<li>探索自监督学习和弱监督学习方法，利用大量未标注数据进行预训练，从而降低对高质量标注数据的依赖。</li>
</ul>
</li>
<li><p><strong>提升实时性能</strong>：</p>
<ul>
<li>开发更加高效的Transformer变种，结合知识蒸馏技术，推动姿态估计算法在移动设备上的实时应用。</li>
</ul>
</li>
<li><p><strong>优化手部细节估计</strong>：</p>
<ul>
<li>增加专门针对手部动作的数据集，并设计更精细的手部特征提取模块，以提高手部姿态估计的准确性。</li>
</ul>
</li>
<li><p><strong>增强模型的通用性和适应性</strong>：</p>
<ul>
<li>设计更具通用性的模型结构，使其能够在不同应用场景中保持较高的性能，同时减小对特定环境条件（如重力方向）的依赖。</li>
</ul>
</li>
</ol>
<hr>
<p>通过对上述三个SOTA方法的深入分析可以看出，Transformer架构凭借其强大的全局建模能力和灵活性，在人体姿态估计领域展现出巨大潜力。未来的研究将进一步探索如何结合多种先进技术，推动该领域的持续进步。 </p>

    </article>
    <!-- license -->
        <div class="license-wrapper">
            <p>Author：<a href="https://graphicyan.github.io">Yan Zhang</a>
            <p>Link：<a href="https://graphicyan.github.io/2024/10/05/Transformer-Based-HPE-SOTA/">https://graphicyan.github.io/2024/10/05/Transformer-Based-HPE-SOTA/</a>
            <p>Publish date：<a href="https://graphicyan.github.io/2024/10/05/Transformer-Based-HPE-SOTA/">October 5th 2024, 3:56:53 pm</a>
            <p>Update date：<a href="https://graphicyan.github.io/2024/10/05/Transformer-Based-HPE-SOTA/">July 20th 2025, 10:21:04 pm</a>
            <p>License：本文采用<a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc/4.0/">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可</p>
        </div>
    <!-- paginator -->
    <ul class="post-paginator">
        <li class="next">
                <div class="nextSlogan">Next Post</div>
                <a href="/2024/11/23/Transformer-Based-Add-Hand/" title="基于Transformer的人体姿态重建（四）：引入高精度的手部姿态">
                    <div class="nextTitle">基于Transformer的人体姿态重建（四）：引入高精度的手部姿态</div>
                </a>
        </li>
        <li class="previous">
                <div class="prevSlogan">Previous Post</div>
                <a href="/2024/08/09/Transformer-Based-HPE-Template/" title="基于Transformer的人体姿态重建（二）：实现模板">
                    <div class="prevTitle">基于Transformer的人体姿态重建（二）：实现模板</div>
                </a>
        </li>
    </ul>
    <!-- comment -->
        <div class="post-comment">
            <!-- 来必力 City 版安装代码 -->

            
            
            
            <!-- utteranc评论 -->

            <!-- partial('_partial/comment/changyan') -->
            <!--PC版-->

            
            
            
        </div>
    <!-- timeliness note -->
    <!-- idea from: https://hexo.fluid-dev.com/posts/hexo-injector/#%E6%96%87%E7%AB%A0%E6%97%B6%E6%95%88%E6%80%A7%E6%8F%90%E7%A4%BA -->
    <!-- Mathjax -->
</main>

                <!-- profile -->
            </div>
            <footer class="footer footer-unloaded">
    <!-- social  -->
        <div class="social">
                            <a href="mailto:yanzhang.cg@gmail.com" class="iconfont-archer email" title="email" ></a>
                <a href="https://github.com/GraphicYan" class="iconfont-archer github" target="_blank" title="github"></a>
                <span class="iconfont-archer wechat" title="wechat">
                    <img class="profile-qr" src="/images/wechat.jpg" />
                </span>

        </div>
    <!-- powered by Hexo  -->
    <div class="copyright">
        <span id="hexo-power">Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></span><span class="iconfont-archer power">&#xe635;</span><span id="theme-info">theme <a href="https://github.com/fi3ework/hexo-theme-archer" target="_blank">Archer</a></span>
    </div>
    <!-- website approve for Chinese user -->
    <!-- 不蒜子  -->
        <div class="busuanzi-container">
                <span id="busuanzi_container_site_pv">PV: <span id="busuanzi_value_site_pv"></span> :)</span>
        </div>
</footer>

        </div>
        <!-- toc -->
            <div class="toc-wrapper toc-wrapper-loding" style=    top:50vh;
>
                <div class="toc-catalog">
                    <span class="iconfont-archer catalog-icon">&#xe613;</span><span>CATALOG</span>
                </div>
                <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%9F%E5%88%9B%E6%80%A7%E5%A3%B0%E6%98%8E"><span class="toc-number">1.</span> <span class="toc-text">原创性声明</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E5%BC%95%E8%A8%80"><span class="toc-number">2.</span> <span class="toc-text">一、引言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81SOTA%E6%96%B9%E6%B3%95%E8%AF%A6%E8%A7%A3"><span class="toc-number">3.</span> <span class="toc-text">二、SOTA方法详解</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-HMR2-0-2023-05-31"><span class="toc-number">3.1.</span> <span class="toc-text">1. HMR2.0 (2023-05-31)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E5%8E%9F%E7%90%86"><span class="toc-number">3.1.1.</span> <span class="toc-text">核心原理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8A%80%E6%9C%AF%E4%BC%98%E5%8A%BF"><span class="toc-number">3.1.2.</span> <span class="toc-text">技术优势</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9%E5%88%86%E6%9E%90"><span class="toc-number">3.1.3.</span> <span class="toc-text">缺点分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">3.1.4.</span> <span class="toc-text">数据集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E9%85%8D%E7%BD%AE"><span class="toc-number">3.1.5.</span> <span class="toc-text">训练配置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87"><span class="toc-number">3.1.6.</span> <span class="toc-text">性能指标</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Multi-HMR-2024-02-22"><span class="toc-number">3.2.</span> <span class="toc-text">2. Multi-HMR (2024-02-22)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E5%8E%9F%E7%90%86-1"><span class="toc-number">3.2.1.</span> <span class="toc-text">核心原理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8A%80%E6%9C%AF%E4%BC%98%E5%8A%BF-1"><span class="toc-number">3.2.2.</span> <span class="toc-text">技术优势</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9%E5%88%86%E6%9E%90-1"><span class="toc-number">3.2.3.</span> <span class="toc-text">缺点分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86-1"><span class="toc-number">3.2.4.</span> <span class="toc-text">数据集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E9%85%8D%E7%BD%AE-1"><span class="toc-number">3.2.5.</span> <span class="toc-text">训练配置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87-1"><span class="toc-number">3.2.6.</span> <span class="toc-text">性能指标</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-GVHMR-2024-09-10"><span class="toc-number">3.3.</span> <span class="toc-text">3. GVHMR (2024-09-10)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E5%8E%9F%E7%90%86-2"><span class="toc-number">3.3.1.</span> <span class="toc-text">核心原理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8A%80%E6%9C%AF%E4%BC%98%E5%8A%BF-2"><span class="toc-number">3.3.2.</span> <span class="toc-text">技术优势</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86-2"><span class="toc-number">3.3.3.</span> <span class="toc-text">数据集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E9%85%8D%E7%BD%AE-2"><span class="toc-number">3.3.4.</span> <span class="toc-text">训练配置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87-2"><span class="toc-number">3.3.5.</span> <span class="toc-text">性能指标</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9%E5%88%86%E6%9E%90-2"><span class="toc-number">3.3.6.</span> <span class="toc-text">缺点分析</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E7%BB%BC%E5%90%88%E6%AF%94%E8%BE%83"><span class="toc-number">3.4.</span> <span class="toc-text">4.综合比较</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81Transformer%E6%9E%B6%E6%9E%84%E7%9A%84%E4%BC%98%E5%8A%BF"><span class="toc-number">4.</span> <span class="toc-text">三、Transformer架构的优势</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%85%A8%E5%B1%80%E4%BF%A1%E6%81%AF%E6%8D%95%E8%8E%B7"><span class="toc-number">4.1.</span> <span class="toc-text">1. 全局信息捕获</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E6%95%88%E7%8E%87"><span class="toc-number">4.2.</span> <span class="toc-text">2. 并行计算效率</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E7%81%B5%E6%B4%BB%E7%9A%84%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E7%BB%93%E6%9E%84"><span class="toc-number">4.3.</span> <span class="toc-text">3. 灵活的输入输出结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E8%87%AA%E7%9B%91%E7%9D%A3%E9%A2%84%E8%AE%AD%E7%BB%83%E6%BD%9C%E5%8A%9B"><span class="toc-number">4.4.</span> <span class="toc-text">4. 自监督预训练潜力</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E6%9C%AA%E6%9D%A5%E5%8F%91%E5%B1%95%E6%96%B9%E5%90%91%E5%8F%8A%E6%94%B9%E8%BF%9B%E5%BB%BA%E8%AE%AE"><span class="toc-number">5.</span> <span class="toc-text">四、未来发展方向及改进建议</span></a></li></ol>
            </div>
        <!-- sidebar -->
        <div class="sidebar sidebar-hide">
    <ul class="sidebar-tabs sidebar-tabs-active-0">
        <li class="sidebar-tab-archives"><span class="iconfont-archer">&#xe67d;</span><span class="tab-name">Archive</span></li>
        <li class="sidebar-tab-tags"><span class="iconfont-archer">&#xe61b;</span><span class="tab-name">Tag</span></li>
        <li class="sidebar-tab-categories"><span class="iconfont-archer">&#xe666;</span><span class="tab-name">Cate</span></li>
    </ul>
    <div class="sidebar-content sidebar-content-show-archive">
        <div class="sidebar-panel-archives">
    <!-- 在 ejs 中将 archive 按照时间排序 -->
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
    <div class="total-and-search">
        <div class="total-archive">
        Total : 22
        </div>
        <!-- search  -->
    </div>
    <div class="post-archive">
            <div class="archive-year"> 2025 </div>
            <ul class="year-list">
        <li class="archive-post-item">
            <span class="archive-post-date">03/28</span>
            <a class="archive-post-title" href="/2025/03/28/ViT-Development/">视觉Transformer发展简史：从ViT到DINOv2的技术演进</a>
        </li>
                </ul>
            <div class="archive-year"> 2024 </div>
            <ul class="year-list">
        <li class="archive-post-item">
            <span class="archive-post-date">11/23</span>
            <a class="archive-post-title" href="/2024/11/23/Transformer-Based-Add-Hand/">基于Transformer的人体姿态重建（四）：引入高精度的手部姿态</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">10/05</span>
            <a class="archive-post-title" href="/2024/10/05/Transformer-Based-HPE-SOTA/">基于Transformer的人体姿态重建（三）：几种SOTA工作的详细解析与未来展望</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">08/09</span>
            <a class="archive-post-title" href="/2024/08/09/Transformer-Based-HPE-Template/">基于Transformer的人体姿态重建（二）：实现模板</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">07/04</span>
            <a class="archive-post-title" href="/2024/07/04/Transformer-Based-HMR/">基于Transformer的人体姿态重建：技术实践与原理详解</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">03/08</span>
            <a class="archive-post-title" href="/2024/03/08/ViT/">Vision Transformer (ViT) 详细技术文档</a>
        </li>
                </ul>
            <div class="archive-year"> 2023 </div>
            <ul class="year-list">
        <li class="archive-post-item">
            <span class="archive-post-date">12/15</span>
            <a class="archive-post-title" href="/2023/12/15/Dcc-Coords/">一图对比众3D引擎的坐标系</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">07/08</span>
            <a class="archive-post-title" href="/2023/07/08/ML-Deformer/">基于机器学习的物理变形器</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">07/02</span>
            <a class="archive-post-title" href="/2023/07/02/Dynamic-Bones/">基于动力学的骨骼动画技术综述</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">06/04</span>
            <a class="archive-post-title" href="/2023/06/04/Simulation-Dynamics-Engine/">仿真及动力学系统综述</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">05/21</span>
            <a class="archive-post-title" href="/2023/05/21/Build-WebRTC-For-MSVC/">编译可以在MSVC中使用的WebRTC库</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">02/21</span>
            <a class="archive-post-title" href="/2023/02/21/Collaborative-Engine/">浅谈端云协同渲染</a>
        </li>
                </ul>
            <div class="archive-year"> 2022 </div>
            <ul class="year-list">
        <li class="archive-post-item">
            <span class="archive-post-date">12/29</span>
            <a class="archive-post-title" href="/2022/12/29/Mixed-Virtual-Production/">混合虚拟制片</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">10/08</span>
            <a class="archive-post-title" href="/2022/10/08/C2BP-Params/">C++生成蓝图的参数类型记录</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">06/29</span>
            <a class="archive-post-title" href="/2022/06/29/Schlick-Approximate/">Schilick的近似方法</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">03/18</span>
            <a class="archive-post-title" href="/2022/03/18/Illumination/">光通量、光强和照度</a>
        </li>
                </ul>
            <div class="archive-year"> 2021 </div>
            <ul class="year-list">
        <li class="archive-post-item">
            <span class="archive-post-date">12/13</span>
            <a class="archive-post-title" href="/2021/12/13/DS-VS-DL/">延迟渲染Vs延迟光照</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">04/17</span>
            <a class="archive-post-title" href="/2021/04/17/Importance-Sampling/">重要性采样</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">04/09</span>
            <a class="archive-post-title" href="/2021/04/09/Monte-Carlo-Path-Tracing/">蒙特卡洛路径追踪</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">04/02</span>
            <a class="archive-post-title" href="/2021/04/02/Cook-Torrance-BRDF/">Cook-Torrance的BRDF</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">03/29</span>
            <a class="archive-post-title" href="/2021/03/29/Learn-Rendering-Function/">理解渲染方程</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">03/22</span>
            <a class="archive-post-title" href="/2021/03/22/Learn-Radiometry/">辐射度量学笔记</a>
        </li>
            </ul>
    </div>
</div>

        <div class="sidebar-panel-tags">
    <div class="sidebar-tags-name">
            <span class="sidebar-tag-name" data-tags="UE">
                <span class="iconfont-archer">&#xe606;</span>
                UE
            </span>
            <span class="sidebar-tag-name" data-tags="Rendering">
                <span class="iconfont-archer">&#xe606;</span>
                Rendering
            </span>
            <span class="sidebar-tag-name" data-tags="3D Engine">
                <span class="iconfont-archer">&#xe606;</span>
                3D Engine
            </span>
            <span class="sidebar-tag-name" data-tags="Animations">
                <span class="iconfont-archer">&#xe606;</span>
                Animations
            </span>
            <span class="sidebar-tag-name" data-tags="Simulation">
                <span class="iconfont-archer">&#xe606;</span>
                Simulation
            </span>
            <span class="sidebar-tag-name" data-tags="AI">
                <span class="iconfont-archer">&#xe606;</span>
                AI
            </span>
    </div>
    <div class="iconfont-archer sidebar-tags-empty">&#xe678;</div>
    <div class="tag-load-fail" style="display: none; color: #ccc; font-size: 0.6rem;">
        缺失模块，请参考主题文档进行安装配置：https://github.com/fi3ework/hexo-theme-archer#%E5%AE%89%E8%A3%85%E4%B8%BB%E9%A2%98
    </div> 
    <div class="sidebar-tags-list"></div>
</div>

        <div class="sidebar-panel-categories">
    <div class="sidebar-categories-name">
    </div>
    <div class="iconfont-archer sidebar-categories-empty">&#xe678;</div>
    <div class="sidebar-categories-list"></div>
</div>

    </div>
</div>

        <!-- site-meta -->
        <script>
    var siteMetaRoot = "/"
    if (siteMetaRoot === "undefined") {
        siteMetaRoot = '/'
    }
    var siteMeta = {
        url: "https://graphicyan.github.io",
        root: siteMetaRoot,
        author: "Yan Zhang"
    }
</script>

        <!-- import experimental options here -->
        <!-- Custom Font -->

        <!-- main func -->
        <script src="/scripts/main.js"></script>
        <!-- fancybox -->
        <script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.36/dist/fancybox/fancybox.umd.js" onload="window.Fancybox.bind('[data-fancybox]')" defer></script>
        <!-- algolia -->
        <!-- busuanzi -->
            <script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>
        <!-- async load share.js -->
            <script src="/scripts/share.js" async></script>
        <!-- mermaid -->
    </body>
</html>
